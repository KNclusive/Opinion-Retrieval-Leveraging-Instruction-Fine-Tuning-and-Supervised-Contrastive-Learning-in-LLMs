{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-score evaluate datasets\n",
    "!pip install transformers\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install spacy gliner-spacy\n",
    "!pip install matplotlib==3.6.0 plotly\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from evaluate import EvaluationModule, load\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Features, Value\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=userdata.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model and Toknizer\n",
    "my_model_name = f\"Your Model which needs to be evaluated from hugginface or anywhere.\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "my_model, my_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = my_model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(my_model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "System_prompt = \"Below is an instruction that describes an information requirement, paired with a claim that provides context. Write a response that appropriately addresses the instruction based on the given claim.\"\n",
    "Input_prompt = \"### Instruction:\\n{instruction}\\n\\n### Claim:\\n{claim}\\n\\n### Response:\\n{answer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    text = Input_prompt.format(instruction=example['Instruction'], claim=example['Claim'], answer='',)\n",
    "    return {'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_dataset(\"json\", data_files=\"perspectrum_instruction_dataset_v2.jsonl\", split = \"train\")\n",
    "train_data = training_data.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_spacy_config = { \"gliner_model\": \"urchade/gliner_large-v2\",\n",
    "                       \"labels\": [\"Person\", \"Date\", \"Organization\", \"Country\", \"Entity\", \"Politics\", \"Social issues\", \"Technology\", \"Environment\", \"Education\", \"Health\", \"Economics\"],\n",
    "                       \"style\": \"ent\",\n",
    "                       \"threshold\": 0.5,\n",
    "                       \"map_location\": device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 # If you have a GPU, set this to 0; otherwise, set it to -1 for CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "bertscore = load(\"bertscore\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluationMetric(EvaluationModule):\n",
    "    def _info(self):\n",
    "        return evaluate.MetricInfo(\n",
    "            description=\"Custom evaluation metric combining BERTScore, stance, and factual accuracy\",\n",
    "            citation=\"\",\n",
    "            features=Features({\n",
    "                \"predictions\": Value(\"string\"),\n",
    "                \"references\": Value(\"string\"),\n",
    "            })\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references, weights=np.array([0.3, 0.3, 0.4])):\n",
    "        bert_scores = self.calculate_bert_score(predictions, references)\n",
    "        stance_scores = self.calculate_stance_score(predictions, references)\n",
    "        factual_scores = self.calculate_factual_accuracy(predictions, references)\n",
    "\n",
    "        score = np.stack([bert_scores, stance_scores, factual_scores])\n",
    "\n",
    "        return {\n",
    "            \"combined_score\": np.dot(weights, score).mean(),\n",
    "            \"bert_score\": bert_scores.mean(),\n",
    "            \"stance_score\": stance_scores.mean(),\n",
    "            \"factual_accuracy\": factual_scores.mean()\n",
    "        }\n",
    "\n",
    "    def calculate_bert_score(self, predictions, references):\n",
    "        return np.array(bertscore.compute(predictions=predictions, references=references, model_type='microsoft/deberta-xlarge-mnli')['f1'])\n",
    "\n",
    "    def calculate_stance_score(self, predictions, references):\n",
    "        def get_stance(text):\n",
    "            return np.array(list(map(lambda x: x['label'], sentiment_pipeline(text, truncation=True, max_length=512, stride=128, return_all_scores=False))))\n",
    "        pred_stance = get_stance(predictions)\n",
    "        ref_stance = get_stance(references)\n",
    "        assert len(pred_stance) == len(ref_stance), \"Shapes not matching after stance classified.\"\n",
    "\n",
    "        return (get_stance(predictions) == get_stance(references)).astype(int)\n",
    "\n",
    "    def calculate_factual_accuracy(self, predictions, references):\n",
    "        def predict_entities(text):\n",
    "            return list(map(lambda x: set(map(lambda y: y.text, x.ents)), nlp.pipe(text)))\n",
    "\n",
    "        pred_ents = predict_entities(predictions)\n",
    "        ref_ents = predict_entities(references)\n",
    "\n",
    "        return np.array([(len(s1&s2) / len(s1|s2)) if len(s1|s2) > 0 else 0.0 for s1, s2 in zip(pred_ents, ref_ents)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the custom metric\n",
    "custom_metric = CustomEvaluationMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "data_loader = DataLoader(train_data, batch_size=16, pin_memory=True, num_workers=4, shuffle=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "RESPONSE_PATTERN = re.compile(r'### Response:\\n([\\s\\S]*)') #Specific to my used prompt change for your need.\n",
    "\n",
    "def extract_answer_fast(model_output):\n",
    "    match = RESPONSE_PATTERN.search(model_output)\n",
    "    return match.group(1).strip() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_batch_metrics_with_plot(len_epoch, model_name, combined_scores, bert_scores, stance_scores, factual_accuracy_scores):\n",
    "    batch_numbers = list(range(len_epoch))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(batch_numbers, combined_scores, label='Combined Score')\n",
    "    plt.plot(batch_numbers, bert_scores, label='BERT Score')\n",
    "    plt.plot(batch_numbers, stance_scores, label='Stance Score')\n",
    "    plt.plot(batch_numbers, factual_accuracy_scores, label='Factual Accuracy')\n",
    "\n",
    "    plt.xlabel('Batch Number')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Metric Progression Across Batches for {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    wandb.log({\"metric_progression\": wandb.Image(plt)})\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import wandb\n",
    "def evaluate_model(my_model, data_loader, device, name):\n",
    "    wandb.login(key=userdata.get(\"WANDB_TOKEN\"))\n",
    "    wandb.init(project=\"Contrastive_Logs\", name=name)\n",
    "\n",
    "    combined_scores = []\n",
    "    bert_scores = []\n",
    "    stance_scores = []\n",
    "    factual_accuracy_scores = []\n",
    "\n",
    "    for num, batch in enumerate(tqdm(data_loader)):\n",
    "        # Generate predictions\n",
    "        text = batch['text']\n",
    "        inputs = my_tokenizer(text, padding='longest', return_tensors=\"pt\").to(device)\n",
    "        outputs = my_model.generate(**inputs, max_new_tokens=128, pad_token_id=my_tokenizer.eos_token_id)\n",
    "        preds = my_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        final_preds = list(map(extract_answer_fast, preds))\n",
    "\n",
    "        assert type(final_preds) == list, \"generated data not in list format\"\n",
    "        assert len(final_preds) == len(batch['Answer']), \"Issue in model generation\"\n",
    "\n",
    "        # Compute metrics\n",
    "        results = custom_metric.compute(predictions=final_preds, references=batch['Answer'])\n",
    "\n",
    "        wandb.log({\"batch\": num+1,\n",
    "                   \"batch_combined_score\": results['combined_score'],\n",
    "                   \"batch_bert_score\": results['bert_score'],\n",
    "                   \"batch_stance_score\": results['stance_score'],\n",
    "                   \"batch_factual_accuracy\": results['factual_accuracy']\n",
    "        })\n",
    "        combined_scores.append(results['combined_score'])\n",
    "        bert_scores.append(results['bert_score'])\n",
    "        stance_scores.append(results['stance_score'])\n",
    "        factual_accuracy_scores.append(results['factual_accuracy'])\n",
    "\n",
    "    log_batch_metrics_with_plot(len(data_loader), name, combined_scores, bert_scores, stance_scores, factual_accuracy_scores)\n",
    "\n",
    "    wandb.log({\n",
    "        \"final_combined_score\": np.mean(combined_scores),\n",
    "        \"final_bert_score\": np.mean(bert_scores),\n",
    "        \"final_stance_score\": np.mean(stance_scores),\n",
    "        \"final_factual_accuracy\": np.mean(factual_accuracy_scores)\n",
    "    })\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(my_model, data_loader, device, 'PerspectrumInstruct-Contrastive-InputSameAsLabels-Epochs_1-EarlyStop-Grad_Accum-Mistral7B')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
